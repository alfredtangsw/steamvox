{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For project 3, your goal is two-fold:\n",
    "1. Using Reddit's API, collect posts from any 2 subreddits\n",
    "2. Use NLP to train a classifier on which subreddit a given post came from. \n",
    "    - This is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### About the API\n",
    "\n",
    "Reddit's API is fairly straightforward. For example, if I want the posts from [`/r/boardgames`](https://www.reddit.com/r/boardgames), all I have to do is add `.json` to the end of the url: https://www.reddit.com/r/boardgames.json\n",
    "\n",
    "To help you get started, we have a primer video on how to use Reddit's API: https://www.youtube.com/watch?v=5Y3ZE26Ciuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- Gather and prepare your data using the `requests` library.\n",
    "- **Create and compare two models**. One of these must be a Bayes classifier, however the other can be a classifier of your choosing: logistic regression, KNN, SVM, etc.\n",
    "- A Jupyter Notebook with your analysis for a peer audience of data scientists.\n",
    "- An executive summary of the results you found.\n",
    "- A short presentation outlining your process and findings for a semi-technical audience.\n",
    "\n",
    "**Pro Tip 1:** You can find a good example executive summary [here](https://www.proposify.biz/blog/executive-summary).\n",
    "\n",
    "**Pro Tip 2:** Reddit will give you 25 posts **per request**. To get enough data, you'll need to hit Reddit's API **repeatedly** (most likely in a `for` loop). _Be sure to use the `time.sleep()` function at the end of your loop to allow for a break in between requests. **THIS IS CRUCIAL**_\n",
    "\n",
    "**Pro tip 3:** The API will cap you at 1,000 posts for each subreddit (assuming the subreddit has that many posts).\n",
    "\n",
    "**Pro tip 4:** At the end of each loop, be sure to save the results from your scrape as a `csv`: JSON from Reddit > Pandas DataFrame > CSV. That way, if something goes wrong in your loop, you won't lose all your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import time\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subreddit1 = 'https://www.reddit.com/r/personalfinance/.json'\n",
    "#subreddit2 = 'https://www.reddit.com/r/writing/.json'\n",
    "id1 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent':'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postcount(id_no,start_offset=0):\n",
    "    \n",
    "    if id_no!='':\n",
    "        steam_jayson_url = 'http://store.steampowered.com/appreviews/'+str(id_no)+'?json=1&start_offset='+str(start_offset)\n",
    "        steam_jayson = requests.get(steam_jayson_url).json()\n",
    "        s_lng = int(srdt_jayson['metadata']['total_results'])\n",
    "        start_offset+=20\n",
    "        return \n",
    "    \n",
    "    else:\n",
    "        print('Enter a steam app ID!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "req1 = requests.get('http://store.steampowered.com/appreviews/10?json=1&start_offset=0',headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recommendationid': '50887827',\n",
       " 'author': {'steamid': '76561198000788461',\n",
       "  'num_games_owned': 132,\n",
       "  'num_reviews': 15,\n",
       "  'playtime_forever': 6639,\n",
       "  'playtime_last_two_weeks': 0,\n",
       "  'last_played': 1544387799},\n",
       " 'language': 'english',\n",
       " 'review': 'this game taught me how to be a man',\n",
       " 'timestamp_created': 1558967616,\n",
       " 'timestamp_updated': 1558967616,\n",
       " 'voted_up': True,\n",
       " 'votes_up': 16,\n",
       " 'votes_funny': 14,\n",
       " 'weighted_vote_score': '0.65728306770324707',\n",
       " 'comment_count': 0,\n",
       " 'steam_purchase': True,\n",
       " 'received_for_free': False,\n",
       " 'written_during_early_access': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req1['reviews'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(url,headers = {'User-agent':'Mozilla/5.0'},loops=2):\n",
    "    posts = []\n",
    "    names = []\n",
    "    titles = []\n",
    "    aft_name=None\n",
    "\n",
    "    for i in range(loops):\n",
    "        if aft_name==None:\n",
    "            params={}\n",
    "        else:\n",
    "            params={'after':aft_name}\n",
    "\n",
    "        req = requests.get(url,params=params,headers=headers)\n",
    "\n",
    "        if req.status_code == 200:\n",
    "            jayson = req.json()\n",
    "            for p in range(len(jayson['review'])):\n",
    "                names.append(jayson['children'][p]['data']['name'])\n",
    "                titles.append(jayson['data']['children'][p]['data']['title'])\n",
    "                posts.append(jayson['data']['children'][p]['data']['selftext'])\n",
    "                aft_name = jayson['data']['after']\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "            \n",
    "        time.sleep(np.random.randint(3,30))\n",
    "    \n",
    "    posts_df = pd.DataFrame({'names':names,\n",
    "                         'titles':titles,\n",
    "                         'posts':posts},columns = ['names','titles','posts'])\n",
    "    \n",
    "    return posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr1_posts = get_posts(subreddit1,loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr1_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>titles</th>\n",
       "      <th>posts</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bvkm9e</td>\n",
       "      <td>30-Day Challenge #6: Review your investment as...</td>\n",
       "      <td># 30-day challenges\\n\\nWe are pleased to conti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_bywisi</td>\n",
       "      <td>Weekday Help and Victory Thread for the week o...</td>\n",
       "      <td>### If you need help, please check the [PF Wik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_bznn9z</td>\n",
       "      <td>Is it wrong to apply for other jobs to see wha...</td>\n",
       "      <td>Some days I wonder if I would get paid more wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_bzc1x4</td>\n",
       "      <td>always get a debt verification letter!</td>\n",
       "      <td>follow up to the $25 medical debt collection p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_bzk3t7</td>\n",
       "      <td>Do people REALLY have emergency funds with 20k...</td>\n",
       "      <td>Hello hello! Really simple question I'm just h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       names                                             titles  \\\n",
       "0  t3_bvkm9e  30-Day Challenge #6: Review your investment as...   \n",
       "1  t3_bywisi  Weekday Help and Victory Thread for the week o...   \n",
       "2  t3_bznn9z  Is it wrong to apply for other jobs to see wha...   \n",
       "3  t3_bzc1x4             always get a debt verification letter!   \n",
       "4  t3_bzk3t7  Do people REALLY have emergency funds with 20k...   \n",
       "\n",
       "                                               posts  subreddit  \n",
       "0  # 30-day challenges\\n\\nWe are pleased to conti...          1  \n",
       "1  ### If you need help, please check the [PF Wik...          1  \n",
       "2  Some days I wonder if I would get paid more wo...          1  \n",
       "3  follow up to the $25 medical debt collection p...          1  \n",
       "4  Hello hello! Really simple question I'm just h...          1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr1_df = sr1_posts\n",
    "sr1_df['subreddit']=1\n",
    "sr1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "987"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr1_df[sr1_df['posts']!=''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicate rows because Reddit API limits the number of records you can request to 1000\n",
    "sr1_df = sr1_df.drop_duplicates(subset='posts',keep='first')\n",
    "sr1_df = sr1_df[sr1_df['posts']!='']\n",
    "len(sr1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr1_df.to_csv('./datasets/subreddit1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr0_posts=get_posts(subreddit0,loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr0_df = sr0_posts\n",
    "sr0_df['subreddit']=0\n",
    "len(sr0_df[sr0_df['posts']!=''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicate rows\n",
    "sr0_df = sr0_df.drop_duplicates()\n",
    "sr0_df = sr0_df[sr0_df['posts']!='']\n",
    "len(sr0_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr0_df.to_csv('./datasets/subreddit0.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>titles</th>\n",
       "      <th>posts</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bzl3k2</td>\n",
       "      <td>“Every lie we tell incurs a debt to the truth....</td>\n",
       "      <td>Isn’t that what debt is? A lie, saying we can ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_bzgsp7</td>\n",
       "      <td>Baby Step 2 - Second Student Loan Paid Off</td>\n",
       "      <td>Another day, another dollar, another debt paid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_bzovir</td>\n",
       "      <td>Money &amp;amp; dating</td>\n",
       "      <td>Hi DR subreddit! I’m almost done replenishing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_bzkg8w</td>\n",
       "      <td>Cutting up credit cards</td>\n",
       "      <td>My husband and I are on step 2 and I want to c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_bzjwxi</td>\n",
       "      <td>The strength to climb a mountain</td>\n",
       "      <td>New to DR and the baby steps. We have complete...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_bzm41r</td>\n",
       "      <td>BS6’ers out there...how do you keep motivated?</td>\n",
       "      <td>After 30 months of constant chipping away, I f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_bzqd6x</td>\n",
       "      <td>HSA in Baby Step #2</td>\n",
       "      <td>About a week a half ago DR received a question...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3_bzdfg7</td>\n",
       "      <td>A meme thread?</td>\n",
       "      <td>I get pictures and links are not allowed in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3_bzevzw</td>\n",
       "      <td>Dave's paycheck math</td>\n",
       "      <td>Anybody else think Dave underestimates the dif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3_bzbh1t</td>\n",
       "      <td>Looking for a new job: keep paying off debt or...</td>\n",
       "      <td>My husband and I are in baby step to and we’ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t3_bz5gxy</td>\n",
       "      <td>\"Taking out loans is the smartest thing an Ame...</td>\n",
       "      <td>Hey guys - Ramsey fan and practitioner. Long s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t3_bzg0ai</td>\n",
       "      <td>Hypothetical question - BS7ish</td>\n",
       "      <td>So i've been blessed with a raise, and i shoul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t3_bzemct</td>\n",
       "      <td>Roth TSP federal employee question</td>\n",
       "      <td>Any retired or current feds out there? I am 37...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t3_bzbc2d</td>\n",
       "      <td>Best time to budget.</td>\n",
       "      <td>Hey everyone!\\n\\nWhen have you all found out t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t3_bza8gw</td>\n",
       "      <td>How to cancel a CC I never used in the first p...</td>\n",
       "      <td>Hi all, I'm new to Dave Ramsey and to adult li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        names                                             titles  \\\n",
       "0   t3_bzl3k2  “Every lie we tell incurs a debt to the truth....   \n",
       "1   t3_bzgsp7         Baby Step 2 - Second Student Loan Paid Off   \n",
       "2   t3_bzovir                                 Money &amp; dating   \n",
       "3   t3_bzkg8w                            Cutting up credit cards   \n",
       "4   t3_bzjwxi                   The strength to climb a mountain   \n",
       "5   t3_bzm41r     BS6’ers out there...how do you keep motivated?   \n",
       "6   t3_bzqd6x                                HSA in Baby Step #2   \n",
       "7   t3_bzdfg7                                     A meme thread?   \n",
       "8   t3_bzevzw                               Dave's paycheck math   \n",
       "9   t3_bzbh1t  Looking for a new job: keep paying off debt or...   \n",
       "10  t3_bz5gxy  \"Taking out loans is the smartest thing an Ame...   \n",
       "11  t3_bzg0ai                     Hypothetical question - BS7ish   \n",
       "12  t3_bzemct                 Roth TSP federal employee question   \n",
       "13  t3_bzbc2d                               Best time to budget.   \n",
       "14  t3_bza8gw  How to cancel a CC I never used in the first p...   \n",
       "\n",
       "                                                posts  subreddit  \n",
       "0   Isn’t that what debt is? A lie, saying we can ...          0  \n",
       "1   Another day, another dollar, another debt paid...          0  \n",
       "2   Hi DR subreddit! I’m almost done replenishing ...          0  \n",
       "3   My husband and I are on step 2 and I want to c...          0  \n",
       "4   New to DR and the baby steps. We have complete...          0  \n",
       "5   After 30 months of constant chipping away, I f...          0  \n",
       "6   About a week a half ago DR received a question...          0  \n",
       "7   I get pictures and links are not allowed in th...          0  \n",
       "8   Anybody else think Dave underestimates the dif...          0  \n",
       "9   My husband and I are in baby step to and we’ve...          0  \n",
       "10  Hey guys - Ramsey fan and practitioner. Long s...          0  \n",
       "11  So i've been blessed with a raise, and i shoul...          0  \n",
       "12  Any retired or current feds out there? I am 37...          0  \n",
       "13  Hey everyone!\\n\\nWhen have you all found out t...          0  \n",
       "14  Hi all, I'm new to Dave Ramsey and to adult li...          0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr0_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
